{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b20c26",
   "metadata": {},
   "source": [
    "# References\n",
    "- [SumTree introduction in Python](https://adventuresinmachinelearning.com/sumtree-introduction-python/)\n",
    "- [A Deeper Look at Experience Replay](https://arxiv.org/abs/1712.01275)\n",
    "- [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)\n",
    "- [Double Learning and Prioritized Experience Replay](https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/)\n",
    "- [GitHub rlcode: Prioritized Experience Replay](https://github.com/rlcode/per)\n",
    "- https://github.com/google/dopamine/tree/a9911ec0a322f38b9c0ea447e61caa2756a383cf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06758b41",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "838f3721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count, product\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import logger\n",
    "logger.set_level(gym.logger.DISABLED)\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ed8f0",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1b7512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A sum tree data structure.\n",
    "Used for prioritized experience replay. See prioritized_replay_buffer.py\n",
    "and Schaul et al. (2015).\n",
    "\"\"\"\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class SumTree(object):\n",
    "    \"\"\"A sum tree data structure for storing replay priorities.\n",
    "    A sum tree is a complete binary tree whose leaves contain values called\n",
    "    priorities. Internal nodes maintain the sum of the priorities of all leaf\n",
    "    nodes in their subtree.\n",
    "    For capacity = 4, the tree may look like this:\n",
    "               +---+\n",
    "               |2.5|\n",
    "               +-+-+\n",
    "                 |\n",
    "         +-------+--------+\n",
    "         |                |\n",
    "       +-+-+            +-+-+\n",
    "       |1.5|            |1.0|\n",
    "       +-+-+            +-+-+\n",
    "         |                |\n",
    "    +----+----+      +----+----+\n",
    "    |         |      |         |\n",
    "    +-+-+     +-+-+  +-+-+     +-+-+\n",
    "    |0.5|     |1.0|  |0.5|     |0.5|\n",
    "    +---+     +---+  +---+     +---+\n",
    "    This is stored in a list of numpy arrays:\n",
    "    self.nodes = [ [2.5], [1.5, 1], [0.5, 1, 0.5, 0.5] ]\n",
    "    For conciseness, we allocate arrays as powers of two, and pad the excess\n",
    "    elements with zero values.\n",
    "    This is similar to the usual array-based representation of a complete binary\n",
    "    tree, but is a little more user-friendly.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"Creates the sum tree data structure for the given replay capacity.\n",
    "        Args:\n",
    "            capacity: int, the maximum number of elements that can be stored in this\n",
    "            data structure.\n",
    "        Raises:\n",
    "            ValueError: If requested capacity is not positive.\n",
    "        \"\"\"\n",
    "        assert isinstance(capacity, int)\n",
    "        if capacity <= 0:\n",
    "            raise ValueError('Sum tree capacity should be positive. Got: {}'.format(capacity))\n",
    "\n",
    "        self.nodes = []\n",
    "        tree_depth = int(math.ceil(np.log2(capacity)))\n",
    "        level_size = 1\n",
    "        for _ in range(tree_depth + 1):\n",
    "            nodes_at_this_depth = np.zeros(level_size)\n",
    "            self.nodes.append(nodes_at_this_depth)\n",
    "            \n",
    "            level_size *= 2\n",
    "\n",
    "        self.max_recorded_priority = 1\n",
    "\n",
    "    def total(self):\n",
    "        \"\"\"Returns the sum of all priorities stored in this sum tree.\n",
    "        Returns:\n",
    "            float, sum of priorities stored in this sum tree.\n",
    "        \"\"\"\n",
    "        return self.nodes[0][0]\n",
    "    \n",
    "    def sample(self, query_value=None):\n",
    "        \"\"\"Samples an element from the sum tree.\n",
    "        Each element has probability p_i / sum_j p_j of being picked, where p_i is\n",
    "        the (positive) value associated with node i (possibly unnormalized).\n",
    "        Args:\n",
    "            query_value: float in [0, 1], used as the random value to select a\n",
    "            sample. If None, will select one randomly in [0, 1).ipynb_checkpoints/\n",
    "        Returns:\n",
    "            int, a random element from the sum tree.\n",
    "        Raises:\n",
    "            Exception: If the sum tree is empty (i.e. its node values sum to 0), or if\n",
    "            the supplied query_value is larger than the total sum.\n",
    "        \"\"\"\n",
    "        if self.total() == 0.0:\n",
    "            raise Exception('Cannot sample from an empty sum tree.')\n",
    "        \n",
    "        if query_value and (query_value < 0. or query_value > 1.):\n",
    "            raise ValueError('query_value must be in [0, 1].')\n",
    "        \n",
    "        # Sample a value in range [0, R), where R is the value stored at the root.\n",
    "        query_value = random.random() if query_value is None else query_value\n",
    "        query_value *= self.total()\n",
    "        \n",
    "        # Now traverse the sum tree.\n",
    "        node_index = 0\n",
    "        for nodes_at_this_depth in self.nodes[1:]:\n",
    "            # Compute children of previous depth's node.\n",
    "            left_child = node_index * 2\n",
    "            \n",
    "            left_sum = nodes_at_this_depth[left_child]\n",
    "            # Each subtree describes a range [0, a), where a is its value.\n",
    "            if query_value < left_sum:  # Recurse into left subtree.\n",
    "                node_index = left_child\n",
    "            else:  # Recurse into right subtree.\n",
    "                node_index = left_child + 1\n",
    "                # Adjust query to be relative to right subtree.\n",
    "                query_value -= left_sum\n",
    "        \n",
    "        return node_index\n",
    "\n",
    "    def stratified_sample(self, batch_size):\n",
    "        \"\"\"Performs stratified sampling using the sum tree.\n",
    "        Let R be the value at the root (total value of sum tree). This method will\n",
    "        divide [0, R) into batch_size segments, pick a random number from each of\n",
    "        those segments, and use that random number to sample from the sum_tree. This\n",
    "        is as specified in Schaul et al. (2015).\n",
    "        Args:\n",
    "            batch_size: int, the number of strata to use.\n",
    "        Returns:\n",
    "            list of batch_size elements sampled from the sum tree.\n",
    "        Raises:\n",
    "            Exception: If the sum tree is empty (i.e. its node values sum to 0).\n",
    "        \"\"\"\n",
    "        if self.total() == 0.0:\n",
    "            raise Exception('Cannot sample from an empty sum tree.')\n",
    "        \n",
    "        bounds = np.linspace(0., 1., batch_size + 1)\n",
    "        assert len(bounds) == batch_size + 1\n",
    "        segments = [(bounds[i], bounds[i+1]) for i in range(batch_size)]\n",
    "        query_values = [random.uniform(x[0], x[1]) for x in segments]\n",
    "        return [self.sample(query_value=x) for x in query_values]\n",
    "\n",
    "    def get(self, node_index):\n",
    "        \"\"\"Returns the value of the leaf node corresponding to the index.\n",
    "        Args:\n",
    "            node_index: The index of the leaf node.\n",
    "        Returns:\n",
    "            The value of the leaf node.\n",
    "        \"\"\"\n",
    "        return self.nodes[-1][node_index]\n",
    "    \n",
    "    def set(self, node_index, value):\n",
    "        \"\"\"Sets the value of a leaf node and updates internal nodes accordingly.\n",
    "        This operation takes O(log(capacity)).\n",
    "        Args:\n",
    "            node_index: int, the index of the leaf node to be updated.\n",
    "            value: float, the value which we assign to the node. This value must be\n",
    "            nonnegative. Setting value = 0 will cause the element to never be\n",
    "            sampled.\n",
    "        Raises:\n",
    "            ValueError: If the given value is negative.\n",
    "        \"\"\"\n",
    "        if value < 0.0:\n",
    "            raise ValueError('Sum tree values should be nonnegative. Got {}'.\n",
    "                             format(value))\n",
    "        self.max_recorded_priority = max(value, self.max_recorded_priority)\n",
    "        \n",
    "        self.nodes[-1][node_index] = value\n",
    "        for nodes_in_parent_layer, nodes_in_child_layer in zip(reversed(self.nodes[:-1]), reversed(self.nodes[1:])):\n",
    "            # Note: Adding a delta leads to some intolerable numerical inaccuracies.\n",
    "            node_index //= 2\n",
    "            nodes_in_parent_layer[node_index] = nodes_in_child_layer[2*node_index] + nodes_in_child_layer[2*node_index+1]\n",
    "        \n",
    "        assert node_index == 0, ('Sum tree traversal failed, final node index '\n",
    "                                 'is not 0.')\n",
    "\n",
    "class ReplayBuffer:\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "    beta = 0.4\n",
    "    beta_increment_per_sampling = 0.001\n",
    "    \n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = int(buffer_size)\n",
    "        self.buffer = []\n",
    "        self.index = 0\n",
    "        self.sum_tree = SumTree(self.buffer_size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def _get_priority(self, error):\n",
    "        return (np.abs(error) + self.e) ** self.a\n",
    "    \n",
    "    def add(self, error, state, action, reward, next_state, done):\n",
    "        priority = self._get_priority(error)\n",
    "        self.sum_tree.set(self.index, priority)\n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.index] = (state, action, reward, next_state, done)\n",
    "        self.index = (self.index + 1) % self.buffer_size\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        indices = self.sum_tree.stratified_sample(batch_size)\n",
    "        priorities = [self.sum_tree.get(index) for index in indices]\n",
    "        batch = [self.buffer[index] for index in indices]\n",
    "        \n",
    "        self.beta = np.min([1, self.beta + self.beta_increment_per_sampling])\n",
    "        \n",
    "        sampling_probabilities = priorities / self.sum_tree.total()\n",
    "        is_weight = np.power(self.buffer_size * sampling_probabilities, -self.beta)\n",
    "        is_weight /= is_weight.max()\n",
    "        \n",
    "        return indices, batch, is_weight\n",
    "    \n",
    "    def update(self, index, error):\n",
    "        priority = self._get_priority(error)\n",
    "        self.sum_tree.set(index, priority)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ccb946",
   "metadata": {},
   "source": [
    "# CartPole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40f6b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bbbcce",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9817958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e41dc",
   "metadata": {},
   "source": [
    "# DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45ee9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, config, nn):\n",
    "        self.step = 0\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.replay_buffer = ReplayBuffer(config[\"buffer_size\"])\n",
    "        self.n_gradient_steps = config[\"n_gradient_steps\"]\n",
    "        self.n_actions = config[\"n_actions\"]\n",
    "        self.epsilon = config[\"epsilon_max\"]\n",
    "        self.epsilon_max = config[\"epsilon_max\"]\n",
    "        self.epsilon_min = config[\"epsilon_min\"]\n",
    "        self.epsilon_decay = config[\"epsilon_decay\"]\n",
    "        self.nn = nn.to(device)\n",
    "        self.target_nn = deepcopy(self.nn).to(device)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.optimizer = optim.RMSprop(self.nn.parameters(), lr=config[\"learning_rate\"])\n",
    "        self.option = config[\"option\"]\n",
    "        if \"beta\" in config:\n",
    "            self.beta = config[\"beta\"]\n",
    "            self.update_target_nn = self.smooth_update_target_nn\n",
    "        elif \"C\" in config:\n",
    "            self.C = config[\"C\"]\n",
    "            self.update_target_nn = self.periodic_update_target_nn\n",
    "        else:\n",
    "            self.update_target_nn = (lambda: None)\n",
    "            self.option = 0\n",
    "    \n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        state_action_values = self.nn(state.unsqueeze(0)).gather(1, action.unsqueeze(0))\n",
    "        if self.option == 2:\n",
    "            # Double Q-Learning\n",
    "            next_state_action_values = self.target_nn(next_state.unsqueeze(0)).gather(1, self.nn(next_state.unsqueeze(0)).max(1)[1].unsqueeze(1)).detach()\n",
    "        elif self.option == 1:\n",
    "            # Target\n",
    "            next_state_action_values = self.target_nn(next_state.unsqueeze(0)).max(1)[0].unsqueeze(1).detach()\n",
    "        else:\n",
    "            # Vanilla\n",
    "            next_state_action_values = self.nn(next_state.unsqueeze(0)).max(1)[0].unsqueeze(1).detach()\n",
    "        expected_state_action_values = reward.unsqueeze(0) + self.gamma * next_state_action_values * (1 - done.unsqueeze(0))\n",
    "        error = (state_action_values - expected_state_action_values).data.numpy()\n",
    "        \n",
    "        self.replay_buffer.add(error, state, action, reward, next_state, done)\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * math.exp(-1. * self.step / self.epsilon_decay)\n",
    "    \n",
    "    def epsilon_greedy_action(self, state):\n",
    "        if random.random() > self.epsilon:\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.nn(torch.Tensor(state))).item()\n",
    "        else:\n",
    "            return torch.tensor(random.randrange(self.n_actions), device=device, dtype=torch.long).item()\n",
    "    \n",
    "    def greedy_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(self.nn(torch.Tensor(state))).item()\n",
    "    \n",
    "    def gradient_step(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        indices, batch, is_weights = self.replay_buffer.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*batch))\n",
    "        \n",
    "        state_batch = torch.stack(batch.state)\n",
    "        action_batch = torch.stack(batch.action)\n",
    "        reward_batch = torch.stack(batch.reward)\n",
    "        next_state_batch = torch.stack(batch.next_state)\n",
    "        done_batch = torch.stack(batch.done)\n",
    "        \n",
    "        state_action_values = self.nn(state_batch).gather(1, action_batch)\n",
    "        if self.option == 2:\n",
    "            # Double Q-Learning\n",
    "            next_state_action_values = self.target_nn(next_state_batch).gather(1, self.nn(next_state_batch).max(1)[1].unsqueeze(1)).detach()\n",
    "        elif self.option == 1:\n",
    "            # Target\n",
    "            next_state_action_values = self.target_nn(next_state_batch).max(1)[0].unsqueeze(1).detach()\n",
    "        else:\n",
    "            # Vanilla\n",
    "            next_state_action_values = self.nn(next_state_batch).max(1)[0].unsqueeze(1).detach()\n",
    "        expected_state_action_values = reward_batch + self.gamma * next_state_action_values * (1 - done_batch)\n",
    "        \n",
    "        errors = (state_action_values - expected_state_action_values).data.numpy()\n",
    "        for i in range(self.batch_size):\n",
    "            index = indices[i]\n",
    "            self.replay_buffer.update(index, errors[i])\n",
    "        \n",
    "        loss = self.criterion(state_action_values, expected_state_action_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.update_target_nn()\n",
    "    \n",
    "    def periodic_update_target_nn(self):\n",
    "        if self.step % self.C == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())  \n",
    "    \n",
    "    def smooth_update_target_nn(self):\n",
    "        for param_nn, param_target_nn in zip(self.nn.parameters(), self.target_nn.parameters()):\n",
    "            param_target_nn.data.copy_(param_nn * self.beta + param_target_nn * (1 - self.beta))\n",
    "    \n",
    "    def train(self, env, n_episodes):\n",
    "        episode_return_list = []\n",
    "        for i_episode in range(1, n_episodes+1):\n",
    "            episode_return = 0\n",
    "            state = env.reset()\n",
    "            for t in count():\n",
    "                self.update_epsilon()\n",
    "                action = self.epsilon_greedy_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_return += reward\n",
    "                \n",
    "                self.append_sample(torch.Tensor(state), torch.tensor([action], dtype=torch.long), torch.tensor([reward], dtype=torch.float), torch.Tensor(next_state), torch.tensor([int(done)], dtype=torch.long))\n",
    "                state = next_state\n",
    "                self.step += 1\n",
    "                \n",
    "                for _ in range(self.n_gradient_steps):\n",
    "                    self.gradient_step()\n",
    "                \n",
    "                if done:\n",
    "                    episode_return_list.append(episode_return)\n",
    "                    print(\"Episode {:4d} : {:4d} steps | epsilon = {:4.2f} | return = {:.1f}\".format(i_episode, t+1, self.epsilon, episode_return))\n",
    "                    break\n",
    "        return episode_return_list\n",
    "    \n",
    "    def test(self, env, step_max):\n",
    "        episode_return = 0\n",
    "        state = env.reset()\n",
    "        for t in count():\n",
    "            env.render()\n",
    "            action = self.greedy_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_return += reward\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "            if done or t+1 >= step_max:\n",
    "                env.close()\n",
    "                return episode_return\n",
    "    \n",
    "    def save(self, name):\n",
    "        torch.save(self.nn.state_dict(), \"trained_nn/{}.pt\".format(name))\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.nn.load_state_dict(torch.load(\"trained_nn/{}.pt\".format(name)))\n",
    "\n",
    "config = {\"gamma\": 0.95,\n",
    "          \"batch_size\": 8,\n",
    "          \"buffer_size\": 1e6,\n",
    "          \"n_gradient_steps\": 1,\n",
    "          \"n_actions\": 2,\n",
    "          \"learning_rate\": 0.001,\n",
    "          \"epsilon_max\": 1.,\n",
    "          \"epsilon_min\": 0.01,\n",
    "          \"epsilon_decay\": 5000,\n",
    "          \"C\": 100,\n",
    "          \"option\": 1}\n",
    "\n",
    "dqn = DQN()\n",
    "agent = Agent(config, dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aac08c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    1 :   15 steps | epsilon = 1.00 | return = 15.0\n",
      "Episode    2 :   12 steps | epsilon = 0.99 | return = 12.0\n",
      "Episode    3 :   31 steps | epsilon = 0.99 | return = 31.0\n",
      "Episode    4 :   14 steps | epsilon = 0.99 | return = 14.0\n",
      "Episode    5 :   16 steps | epsilon = 0.98 | return = 16.0\n",
      "Episode    6 :   38 steps | epsilon = 0.98 | return = 38.0\n",
      "Episode    7 :   18 steps | epsilon = 0.97 | return = 18.0\n",
      "Episode    8 :   23 steps | epsilon = 0.97 | return = 23.0\n",
      "Episode    9 :   34 steps | epsilon = 0.96 | return = 34.0\n",
      "Episode   10 :   20 steps | epsilon = 0.96 | return = 20.0\n",
      "Episode   11 :   27 steps | epsilon = 0.95 | return = 27.0\n",
      "Episode   12 :   40 steps | epsilon = 0.94 | return = 40.0\n",
      "Episode   13 :   17 steps | epsilon = 0.94 | return = 17.0\n",
      "Episode   14 :   31 steps | epsilon = 0.94 | return = 31.0\n",
      "Episode   15 :   27 steps | epsilon = 0.93 | return = 27.0\n",
      "Episode   16 :   20 steps | epsilon = 0.93 | return = 20.0\n",
      "Episode   17 :   32 steps | epsilon = 0.92 | return = 32.0\n",
      "Episode   18 :   35 steps | epsilon = 0.91 | return = 35.0\n",
      "Episode   19 :   58 steps | epsilon = 0.90 | return = 58.0\n",
      "Episode   20 :   10 steps | epsilon = 0.90 | return = 10.0\n",
      "Episode   21 :   20 steps | epsilon = 0.90 | return = 20.0\n",
      "Episode   22 :   17 steps | epsilon = 0.90 | return = 17.0\n",
      "Episode   23 :   45 steps | epsilon = 0.89 | return = 45.0\n",
      "Episode   24 :   20 steps | epsilon = 0.88 | return = 20.0\n",
      "Episode   25 :   11 steps | epsilon = 0.88 | return = 11.0\n",
      "Episode   26 :   12 steps | epsilon = 0.88 | return = 12.0\n",
      "Episode   27 :   13 steps | epsilon = 0.88 | return = 13.0\n",
      "Episode   28 :   10 steps | epsilon = 0.88 | return = 10.0\n",
      "Episode   29 :   13 steps | epsilon = 0.87 | return = 13.0\n",
      "Episode   30 :   24 steps | epsilon = 0.87 | return = 24.0\n",
      "Episode   31 :   11 steps | epsilon = 0.87 | return = 11.0\n",
      "Episode   32 :    8 steps | epsilon = 0.87 | return = 8.0\n",
      "Episode   33 :   20 steps | epsilon = 0.86 | return = 20.0\n",
      "Episode   34 :   11 steps | epsilon = 0.86 | return = 11.0\n",
      "Episode   35 :   10 steps | epsilon = 0.86 | return = 10.0\n",
      "Episode   36 :   23 steps | epsilon = 0.86 | return = 23.0\n",
      "Episode   37 :   18 steps | epsilon = 0.85 | return = 18.0\n",
      "Episode   38 :   36 steps | epsilon = 0.85 | return = 36.0\n",
      "Episode   39 :   13 steps | epsilon = 0.84 | return = 13.0\n",
      "Episode   40 :   13 steps | epsilon = 0.84 | return = 13.0\n",
      "Episode   41 :    9 steps | epsilon = 0.84 | return = 9.0\n",
      "Episode   42 :   33 steps | epsilon = 0.84 | return = 33.0\n",
      "Episode   43 :   14 steps | epsilon = 0.83 | return = 14.0\n",
      "Episode   44 :   32 steps | epsilon = 0.83 | return = 32.0\n",
      "Episode   45 :   28 steps | epsilon = 0.82 | return = 28.0\n",
      "Episode   46 :   16 steps | epsilon = 0.82 | return = 16.0\n",
      "Episode   47 :   15 steps | epsilon = 0.82 | return = 15.0\n",
      "Episode   48 :   11 steps | epsilon = 0.82 | return = 11.0\n",
      "Episode   49 :   22 steps | epsilon = 0.81 | return = 22.0\n",
      "Episode   50 :   12 steps | epsilon = 0.81 | return = 12.0\n",
      "Episode   51 :   12 steps | epsilon = 0.81 | return = 12.0\n",
      "Episode   52 :   17 steps | epsilon = 0.81 | return = 17.0\n",
      "Episode   53 :   29 steps | epsilon = 0.80 | return = 29.0\n",
      "Episode   54 :   37 steps | epsilon = 0.80 | return = 37.0\n",
      "Episode   55 :   13 steps | epsilon = 0.79 | return = 13.0\n",
      "Episode   56 :   15 steps | epsilon = 0.79 | return = 15.0\n",
      "Episode   57 :   12 steps | epsilon = 0.79 | return = 12.0\n",
      "Episode   58 :   14 steps | epsilon = 0.79 | return = 14.0\n",
      "Episode   59 :   13 steps | epsilon = 0.79 | return = 13.0\n",
      "Episode   60 :   40 steps | epsilon = 0.78 | return = 40.0\n",
      "Episode   61 :   17 steps | epsilon = 0.78 | return = 17.0\n",
      "Episode   62 :   16 steps | epsilon = 0.77 | return = 16.0\n",
      "Episode   63 :   28 steps | epsilon = 0.77 | return = 28.0\n",
      "Episode   64 :   73 steps | epsilon = 0.76 | return = 73.0\n",
      "Episode   65 :   64 steps | epsilon = 0.75 | return = 64.0\n",
      "Episode   66 :   41 steps | epsilon = 0.74 | return = 41.0\n",
      "Episode   67 :   60 steps | epsilon = 0.73 | return = 60.0\n",
      "Episode   68 :   17 steps | epsilon = 0.73 | return = 17.0\n",
      "Episode   69 :   64 steps | epsilon = 0.72 | return = 64.0\n",
      "Episode   70 :   88 steps | epsilon = 0.71 | return = 88.0\n",
      "Episode   71 :  194 steps | epsilon = 0.68 | return = 194.0\n",
      "Episode   72 :   39 steps | epsilon = 0.68 | return = 39.0\n",
      "Episode   73 :   13 steps | epsilon = 0.68 | return = 13.0\n",
      "Episode   74 :   18 steps | epsilon = 0.67 | return = 18.0\n",
      "Episode   75 :   24 steps | epsilon = 0.67 | return = 24.0\n",
      "Episode   76 :   36 steps | epsilon = 0.67 | return = 36.0\n",
      "Episode   77 :   44 steps | epsilon = 0.66 | return = 44.0\n",
      "Episode   78 :   17 steps | epsilon = 0.66 | return = 17.0\n",
      "Episode   79 :   20 steps | epsilon = 0.66 | return = 20.0\n",
      "Episode   80 :   18 steps | epsilon = 0.65 | return = 18.0\n",
      "Episode   81 :   78 steps | epsilon = 0.64 | return = 78.0\n",
      "Episode   82 :   54 steps | epsilon = 0.64 | return = 54.0\n",
      "Episode   83 :   20 steps | epsilon = 0.63 | return = 20.0\n",
      "Episode   84 :   18 steps | epsilon = 0.63 | return = 18.0\n",
      "Episode   85 :   58 steps | epsilon = 0.63 | return = 58.0\n",
      "Episode   86 :   64 steps | epsilon = 0.62 | return = 64.0\n",
      "Episode   87 :   96 steps | epsilon = 0.61 | return = 96.0\n",
      "Episode   88 :   52 steps | epsilon = 0.60 | return = 52.0\n",
      "Episode   89 :   94 steps | epsilon = 0.59 | return = 94.0\n",
      "Episode   90 :   64 steps | epsilon = 0.58 | return = 64.0\n",
      "Episode   91 :   84 steps | epsilon = 0.57 | return = 84.0\n",
      "Episode   92 :   98 steps | epsilon = 0.56 | return = 98.0\n",
      "Episode   93 :   45 steps | epsilon = 0.56 | return = 45.0\n",
      "Episode   94 :   21 steps | epsilon = 0.55 | return = 21.0\n",
      "Episode   95 :   51 steps | epsilon = 0.55 | return = 51.0\n",
      "Episode   96 :  114 steps | epsilon = 0.54 | return = 114.0\n",
      "Episode   97 :   27 steps | epsilon = 0.53 | return = 27.0\n",
      "Episode   98 :   63 steps | epsilon = 0.53 | return = 63.0\n",
      "Episode   99 :  163 steps | epsilon = 0.51 | return = 163.0\n",
      "Episode  100 :  156 steps | epsilon = 0.49 | return = 156.0\n"
     ]
    }
   ],
   "source": [
    "episode_return_list = agent.train(env, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4302c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleSwingUp(gym.Wrapper):\n",
    "    def __init__(self, env, **kwargs):\n",
    "        super(CartPoleSwingUp, self).__init__(env, **kwargs)\n",
    "        self.theta_dot_threshold = 4*np.pi\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.env.state = [0, 0, np.pi, 0] + super().reset()\n",
    "        self.env.env.steps_beyond_done = None\n",
    "        return np.array(self.env.env.state)\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, _ = super().step(action)\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "        \n",
    "        done = x < -self.x_threshold \\\n",
    "               or x > self.x_threshold \\\n",
    "               or theta_dot < -self.theta_dot_threshold \\\n",
    "               or theta_dot > self.theta_dot_threshold\n",
    "        \n",
    "        if done:\n",
    "            # game over\n",
    "            reward = -10.\n",
    "            if self.steps_beyond_done is None:\n",
    "                self.steps_beyond_done = 0\n",
    "            else:\n",
    "                logger.warn(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
    "                self.steps_beyond_done += 1\n",
    "        else:\n",
    "            if -self.theta_threshold_radians < theta and theta < self.theta_threshold_radians:\n",
    "                # pole upright\n",
    "                reward = 1.\n",
    "            else:\n",
    "                # pole swinging\n",
    "                reward = 0.\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "env = CartPoleSwingUp(gym.make('CartPole-v1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dbe7e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"gamma\": 0.95,\n",
    "          \"batch_size\": 64,\n",
    "          \"buffer_size\": 1e6,\n",
    "          \"n_gradient_steps\": 1,\n",
    "          \"n_actions\": 2,\n",
    "          \"learning_rate\": 0.001,\n",
    "          \"epsilon_max\": 1.,\n",
    "          \"epsilon_min\": 0.01,\n",
    "          \"epsilon_decay\": 5000,\n",
    "          \"C\": 100,\n",
    "          \"option\": 2}\n",
    "\n",
    "dqn = DQN()\n",
    "agent = Agent(config, dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7556536d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    1 :   79 steps | epsilon = 0.98 | return = -10.0\n",
      "Episode    2 :  134 steps | epsilon = 0.96 | return = -10.0\n",
      "Episode    3 :  253 steps | epsilon = 0.91 | return = -10.0\n",
      "Episode    4 :  139 steps | epsilon = 0.89 | return = -10.0\n",
      "Episode    5 :   70 steps | epsilon = 0.88 | return = -10.0\n",
      "Episode    6 :  307 steps | epsilon = 0.82 | return = -10.0\n",
      "Episode    7 :   83 steps | epsilon = 0.81 | return = -10.0\n",
      "Episode    8 :  411 steps | epsilon = 0.75 | return = -10.0\n",
      "Episode    9 :  239 steps | epsilon = 0.71 | return = 3.0\n",
      "Episode   10 :  182 steps | epsilon = 0.69 | return = -10.0\n",
      "Episode   11 :  121 steps | epsilon = 0.67 | return = -10.0\n",
      "Episode   12 :  190 steps | epsilon = 0.65 | return = -10.0\n",
      "Episode   13 :  240 steps | epsilon = 0.62 | return = -10.0\n",
      "Episode   14 :  179 steps | epsilon = 0.60 | return = -10.0\n",
      "Episode   15 :  108 steps | epsilon = 0.58 | return = -10.0\n",
      "Episode   16 :  229 steps | epsilon = 0.56 | return = -10.0\n",
      "Episode   17 :  219 steps | epsilon = 0.53 | return = -10.0\n",
      "Episode   18 :  330 steps | epsilon = 0.50 | return = -10.0\n",
      "Episode   19 :  545 steps | epsilon = 0.45 | return = -10.0\n",
      "Episode   20 :  251 steps | epsilon = 0.43 | return = -10.0\n",
      "Episode   21 :  367 steps | epsilon = 0.40 | return = -10.0\n",
      "Episode   22 :  373 steps | epsilon = 0.37 | return = -10.0\n",
      "Episode   23 : 1017 steps | epsilon = 0.30 | return = -10.0\n",
      "Episode   24 :  402 steps | epsilon = 0.28 | return = -10.0\n",
      "Episode   25 :  234 steps | epsilon = 0.27 | return = -5.0\n",
      "Episode   26 :  214 steps | epsilon = 0.26 | return = -7.0\n",
      "Episode   27 :   45 steps | epsilon = 0.26 | return = -10.0\n",
      "Episode   28 :   48 steps | epsilon = 0.25 | return = -10.0\n",
      "Episode   29 :   48 steps | epsilon = 0.25 | return = -10.0\n",
      "Episode   30 :   52 steps | epsilon = 0.25 | return = -10.0\n",
      "Episode   31 :   49 steps | epsilon = 0.25 | return = -10.0\n",
      "Episode   32 :   53 steps | epsilon = 0.24 | return = -10.0\n",
      "Episode   33 :   51 steps | epsilon = 0.24 | return = -10.0\n",
      "Episode   34 :  137 steps | epsilon = 0.24 | return = -6.0\n",
      "Episode   35 :  119 steps | epsilon = 0.23 | return = -10.0\n",
      "Episode   36 :  111 steps | epsilon = 0.23 | return = -10.0\n",
      "Episode   37 :  137 steps | epsilon = 0.22 | return = -6.0\n",
      "Episode   38 :  141 steps | epsilon = 0.21 | return = -6.0\n",
      "Episode   39 :  134 steps | epsilon = 0.21 | return = -6.0\n",
      "Episode   40 :  139 steps | epsilon = 0.20 | return = -1.0\n",
      "Episode   41 :  144 steps | epsilon = 0.20 | return = 1.0\n",
      "Episode   42 :  142 steps | epsilon = 0.19 | return = -5.0\n",
      "Episode   43 : 1044 steps | epsilon = 0.16 | return = -10.0\n",
      "Episode   44 :  103 steps | epsilon = 0.15 | return = -8.0\n",
      "Episode   45 :  144 steps | epsilon = 0.15 | return = 7.0\n",
      "Episode   46 :  101 steps | epsilon = 0.15 | return = -10.0\n",
      "Episode   47 :   94 steps | epsilon = 0.15 | return = -10.0\n",
      "Episode   48 :   94 steps | epsilon = 0.14 | return = -10.0\n",
      "Episode   49 :  454 steps | epsilon = 0.13 | return = 17.0\n",
      "Episode   50 :  152 steps | epsilon = 0.13 | return = -2.0\n",
      "Episode   51 :  156 steps | epsilon = 0.12 | return = -10.0\n",
      "Episode   52 :  146 steps | epsilon = 0.12 | return = 1.0\n",
      "Episode   53 :  161 steps | epsilon = 0.12 | return = 4.0\n",
      "Episode   54 :  583 steps | epsilon = 0.11 | return = -3.0\n",
      "Episode   55 :  147 steps | epsilon = 0.10 | return = -6.0\n",
      "Episode   56 :  156 steps | epsilon = 0.10 | return = -5.0\n",
      "Episode   57 :  150 steps | epsilon = 0.10 | return = -6.0\n",
      "Episode   58 :  131 steps | epsilon = 0.09 | return = -10.0\n",
      "Episode   59 :  160 steps | epsilon = 0.09 | return = -6.0\n",
      "Episode   60 :  152 steps | epsilon = 0.09 | return = -5.0\n",
      "Episode   61 :  163 steps | epsilon = 0.09 | return = -4.0\n",
      "Episode   62 :  370 steps | epsilon = 0.08 | return = -10.0\n",
      "Episode   63 :  271 steps | epsilon = 0.08 | return = -9.0\n",
      "Episode   64 :  107 steps | epsilon = 0.08 | return = -10.0\n",
      "Episode   65 :  111 steps | epsilon = 0.08 | return = -10.0\n",
      "Episode   66 :  170 steps | epsilon = 0.07 | return = -2.0\n",
      "Episode   67 :  108 steps | epsilon = 0.07 | return = -10.0\n",
      "Episode   68 :  391 steps | epsilon = 0.07 | return = -9.0\n",
      "Episode   69 :  418 steps | epsilon = 0.06 | return = 7.0\n",
      "Episode   70 :  161 steps | epsilon = 0.06 | return = -4.0\n",
      "Episode   71 :  163 steps | epsilon = 0.06 | return = -1.0\n",
      "Episode   72 :  156 steps | epsilon = 0.06 | return = -4.0\n",
      "Episode   73 :  165 steps | epsilon = 0.06 | return = -5.0\n",
      "Episode   74 :  349 steps | epsilon = 0.05 | return = -10.0\n",
      "Episode   75 :  176 steps | epsilon = 0.05 | return = 0.0\n",
      "Episode   76 :  267 steps | epsilon = 0.05 | return = -10.0\n",
      "Episode   77 :  161 steps | epsilon = 0.05 | return = -1.0\n",
      "Episode   78 :  106 steps | epsilon = 0.05 | return = -10.0\n",
      "Episode   79 :  104 steps | epsilon = 0.05 | return = -10.0\n",
      "Episode   80 :  160 steps | epsilon = 0.05 | return = -2.0\n",
      "Episode   81 :  176 steps | epsilon = 0.04 | return = 3.0\n",
      "Episode   82 :  371 steps | epsilon = 0.04 | return = -10.0\n",
      "Episode   83 :  172 steps | epsilon = 0.04 | return = -4.0\n",
      "Episode   84 :  101 steps | epsilon = 0.04 | return = -10.0\n",
      "Episode   85 :  181 steps | epsilon = 0.04 | return = -10.0\n",
      "Episode   86 :  196 steps | epsilon = 0.04 | return = 4.0\n",
      "Episode   87 :  190 steps | epsilon = 0.04 | return = -1.0\n",
      "Episode   88 :  188 steps | epsilon = 0.04 | return = -1.0\n",
      "Episode   89 :  132 steps | epsilon = 0.04 | return = -10.0\n",
      "Episode   90 :  194 steps | epsilon = 0.03 | return = -4.0\n",
      "Episode   91 :  175 steps | epsilon = 0.03 | return = 14.0\n",
      "Episode   92 :  173 steps | epsilon = 0.03 | return = -6.0\n",
      "Episode   93 :  176 steps | epsilon = 0.03 | return = -4.0\n",
      "Episode   94 :  183 steps | epsilon = 0.03 | return = 14.0\n",
      "Episode   95 :  152 steps | epsilon = 0.03 | return = -7.0\n",
      "Episode   96 :  180 steps | epsilon = 0.03 | return = 0.0\n",
      "Episode   97 :  445 steps | epsilon = 0.03 | return = -10.0\n",
      "Episode   98 :  170 steps | epsilon = 0.03 | return = -10.0\n",
      "Episode   99 :  170 steps | epsilon = 0.03 | return = -10.0\n",
      "Episode  100 :  173 steps | epsilon = 0.03 | return = -4.0\n",
      "Episode  101 :  198 steps | epsilon = 0.03 | return = 1.0\n",
      "Episode  102 :  209 steps | epsilon = 0.02 | return = 2.0\n",
      "Episode  103 :  155 steps | epsilon = 0.02 | return = -10.0\n",
      "Episode  104 :  355 steps | epsilon = 0.02 | return = -10.0\n",
      "Episode  105 :  184 steps | epsilon = 0.02 | return = -3.0\n",
      "Episode  106 :  190 steps | epsilon = 0.02 | return = 4.0\n",
      "Episode  107 :  174 steps | epsilon = 0.02 | return = 10.0\n",
      "Episode  108 :  193 steps | epsilon = 0.02 | return = 4.0\n",
      "Episode  109 :  206 steps | epsilon = 0.02 | return = 0.0\n",
      "Episode  110 :  140 steps | epsilon = 0.02 | return = -10.0\n",
      "Episode  111 :  205 steps | epsilon = 0.02 | return = 5.0\n",
      "Episode  112 :  754 steps | epsilon = 0.02 | return = -10.0\n",
      "Episode  113 :  101 steps | epsilon = 0.02 | return = -10.0\n",
      "Episode  114 :  250 steps | epsilon = 0.02 | return = -10.0\n",
      "Episode  115 :  148 steps | epsilon = 0.02 | return = -10.0\n",
      "Episode  116 :  159 steps | epsilon = 0.02 | return = -10.0\n",
      "Episode  117 :  101 steps | epsilon = 0.02 | return = -10.0\n",
      "Episode  118 :  276 steps | epsilon = 0.02 | return = -3.0\n",
      "Episode  119 :  184 steps | epsilon = 0.02 | return = -1.0\n",
      "Episode  120 :  293 steps | epsilon = 0.02 | return = -10.0\n",
      "Episode  121 :  173 steps | epsilon = 0.02 | return = -10.0\n",
      "Episode  122 :  182 steps | epsilon = 0.02 | return = -10.0\n",
      "Episode  123 :  168 steps | epsilon = 0.02 | return = -10.0\n",
      "Episode  124 :  281 steps | epsilon = 0.02 | return = -10.0\n",
      "Episode  125 :  175 steps | epsilon = 0.02 | return = -10.0\n",
      "Episode  126 :  339 steps | epsilon = 0.02 | return = -10.0\n",
      "Episode  127 :  127 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  128 :  352 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  129 :  862 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  130 :  186 steps | epsilon = 0.01 | return = -4.0\n",
      "Episode  131 :  908 steps | epsilon = 0.01 | return = 2.0\n",
      "Episode  132 :  194 steps | epsilon = 0.01 | return = -3.0\n",
      "Episode  133 :  367 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  134 :  785 steps | epsilon = 0.01 | return = 0.0\n",
      "Episode  135 :  330 steps | epsilon = 0.01 | return = -4.0\n",
      "Episode  136 :  424 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  137 :  702 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  138 :  148 steps | epsilon = 0.01 | return = -4.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  139 :  392 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  140 :  494 steps | epsilon = 0.01 | return = -1.0\n",
      "Episode  141 :  125 steps | epsilon = 0.01 | return = -5.0\n",
      "Episode  142 :  132 steps | epsilon = 0.01 | return = -1.0\n",
      "Episode  143 :  143 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  144 :  140 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  145 :  118 steps | epsilon = 0.01 | return = 3.0\n",
      "Episode  146 :  135 steps | epsilon = 0.01 | return = -1.0\n",
      "Episode  147 :  442 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  148 :  157 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  149 :  375 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  150 :  316 steps | epsilon = 0.01 | return = -4.0\n",
      "Episode  151 :  414 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  152 :  525 steps | epsilon = 0.01 | return = -1.0\n",
      "Episode  153 :  142 steps | epsilon = 0.01 | return = 2.0\n",
      "Episode  154 :  682 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  155 :  504 steps | epsilon = 0.01 | return = 0.0\n",
      "Episode  156 :  167 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  157 :  607 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  158 :  378 steps | epsilon = 0.01 | return = -8.0\n",
      "Episode  159 :  322 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  160 :  215 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  161 :  722 steps | epsilon = 0.01 | return = -6.0\n",
      "Episode  162 :  325 steps | epsilon = 0.01 | return = -3.0\n",
      "Episode  163 :  200 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  164 :  540 steps | epsilon = 0.01 | return = -1.0\n",
      "Episode  165 :  225 steps | epsilon = 0.01 | return = -5.0\n",
      "Episode  166 :  668 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  167 :  592 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  168 :  157 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  169 :  144 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  170 :  381 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  171 :  131 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  172 :  134 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  173 :  143 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  174 :  770 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  175 : 1328 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  176 :  306 steps | epsilon = 0.01 | return = -1.0\n",
      "Episode  177 :  294 steps | epsilon = 0.01 | return = -5.0\n",
      "Episode  178 :  164 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  179 :  271 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  180 :  349 steps | epsilon = 0.01 | return = -3.0\n",
      "Episode  181 :  301 steps | epsilon = 0.01 | return = -5.0\n",
      "Episode  182 :  478 steps | epsilon = 0.01 | return = -7.0\n",
      "Episode  183 :  793 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  184 :  227 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  185 :  253 steps | epsilon = 0.01 | return = -3.0\n",
      "Episode  186 :  692 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  187 :  313 steps | epsilon = 0.01 | return = -7.0\n",
      "Episode  188 :  808 steps | epsilon = 0.01 | return = 19.0\n",
      "Episode  189 :  302 steps | epsilon = 0.01 | return = -7.0\n",
      "Episode  190 :  641 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  191 :  575 steps | epsilon = 0.01 | return = -3.0\n",
      "Episode  192 :  167 steps | epsilon = 0.01 | return = -4.0\n",
      "Episode  193 :  353 steps | epsilon = 0.01 | return = 69.0\n",
      "Episode  194 :  298 steps | epsilon = 0.01 | return = -3.0\n",
      "Episode  195 :  359 steps | epsilon = 0.01 | return = -5.0\n",
      "Episode  196 :  173 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  197 :  243 steps | epsilon = 0.01 | return = 17.0\n",
      "Episode  198 :  283 steps | epsilon = 0.01 | return = -7.0\n",
      "Episode  199 :  483 steps | epsilon = 0.01 | return = -5.0\n",
      "Episode  200 :  296 steps | epsilon = 0.01 | return = 11.0\n",
      "Episode  201 :  409 steps | epsilon = 0.01 | return = 1.0\n",
      "Episode  202 :  230 steps | epsilon = 0.01 | return = -4.0\n",
      "Episode  203 :  329 steps | epsilon = 0.01 | return = -5.0\n",
      "Episode  204 :  206 steps | epsilon = 0.01 | return = 4.0\n",
      "Episode  205 :  365 steps | epsilon = 0.01 | return = 15.0\n",
      "Episode  206 :  234 steps | epsilon = 0.01 | return = -6.0\n",
      "Episode  207 :  272 steps | epsilon = 0.01 | return = 0.0\n",
      "Episode  208 :  234 steps | epsilon = 0.01 | return = -6.0\n",
      "Episode  209 :  253 steps | epsilon = 0.01 | return = -6.0\n",
      "Episode  210 :  259 steps | epsilon = 0.01 | return = -7.0\n",
      "Episode  211 :  200 steps | epsilon = 0.01 | return = 0.0\n",
      "Episode  212 :  182 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  213 :  202 steps | epsilon = 0.01 | return = -5.0\n",
      "Episode  214 :  306 steps | epsilon = 0.01 | return = -7.0\n",
      "Episode  215 :  709 steps | epsilon = 0.01 | return = -8.0\n",
      "Episode  216 :  536 steps | epsilon = 0.01 | return = -5.0\n",
      "Episode  217 :  253 steps | epsilon = 0.01 | return = 11.0\n",
      "Episode  218 :  222 steps | epsilon = 0.01 | return = -4.0\n",
      "Episode  219 :  164 steps | epsilon = 0.01 | return = -7.0\n",
      "Episode  220 :  276 steps | epsilon = 0.01 | return = 38.0\n",
      "Episode  221 :  195 steps | epsilon = 0.01 | return = 0.0\n",
      "Episode  222 :  170 steps | epsilon = 0.01 | return = -5.0\n",
      "Episode  223 :  183 steps | epsilon = 0.01 | return = -1.0\n",
      "Episode  224 :  182 steps | epsilon = 0.01 | return = -7.0\n",
      "Episode  225 :  927 steps | epsilon = 0.01 | return = 21.0\n",
      "Episode  226 :  337 steps | epsilon = 0.01 | return = 130.0\n",
      "Episode  227 :  191 steps | epsilon = 0.01 | return = 5.0\n",
      "Episode  228 :  194 steps | epsilon = 0.01 | return = 5.0\n",
      "Episode  229 :  187 steps | epsilon = 0.01 | return = 6.0\n",
      "Episode  230 :   44 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  231 :  157 steps | epsilon = 0.01 | return = -3.0\n",
      "Episode  232 :  426 steps | epsilon = 0.01 | return = 1.0\n",
      "Episode  233 :  197 steps | epsilon = 0.01 | return = -1.0\n",
      "Episode  234 :  176 steps | epsilon = 0.01 | return = 2.0\n",
      "Episode  235 :  188 steps | epsilon = 0.01 | return = 2.0\n",
      "Episode  236 :  152 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  237 :   57 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  238 :  296 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  239 :  215 steps | epsilon = 0.01 | return = 14.0\n",
      "Episode  240 :  190 steps | epsilon = 0.01 | return = 3.0\n",
      "Episode  241 :  353 steps | epsilon = 0.01 | return = 7.0\n",
      "Episode  242 :  174 steps | epsilon = 0.01 | return = 11.0\n",
      "Episode  243 :  231 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  244 :  299 steps | epsilon = 0.01 | return = 23.0\n",
      "Episode  245 :  263 steps | epsilon = 0.01 | return = -6.0\n",
      "Episode  246 :  420 steps | epsilon = 0.01 | return = 8.0\n",
      "Episode  247 :  238 steps | epsilon = 0.01 | return = 6.0\n",
      "Episode  248 :  219 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  249 :  332 steps | epsilon = 0.01 | return = 124.0\n",
      "Episode  250 :  275 steps | epsilon = 0.01 | return = 29.0\n",
      "Episode  251 :  307 steps | epsilon = 0.01 | return = 46.0\n",
      "Episode  252 :  168 steps | epsilon = 0.01 | return = 5.0\n",
      "Episode  253 :  231 steps | epsilon = 0.01 | return = 1.0\n",
      "Episode  254 :  214 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  255 :  176 steps | epsilon = 0.01 | return = 6.0\n",
      "Episode  256 :  246 steps | epsilon = 0.01 | return = 40.0\n",
      "Episode  257 :  199 steps | epsilon = 0.01 | return = 28.0\n",
      "Episode  258 :  179 steps | epsilon = 0.01 | return = 34.0\n",
      "Episode  259 :  263 steps | epsilon = 0.01 | return = 69.0\n",
      "Episode  260 :  209 steps | epsilon = 0.01 | return = 47.0\n",
      "Episode  261 :  287 steps | epsilon = 0.01 | return = 34.0\n",
      "Episode  262 :  244 steps | epsilon = 0.01 | return = 27.0\n",
      "Episode  263 :  485 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  264 :  233 steps | epsilon = 0.01 | return = 117.0\n",
      "Episode  265 :  473 steps | epsilon = 0.01 | return = 63.0\n",
      "Episode  266 :  200 steps | epsilon = 0.01 | return = 9.0\n",
      "Episode  267 :  279 steps | epsilon = 0.01 | return = 26.0\n",
      "Episode  268 :  238 steps | epsilon = 0.01 | return = 10.0\n",
      "Episode  269 : 3277 steps | epsilon = 0.01 | return = 37.0\n",
      "Episode  270 :  203 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  271 :  410 steps | epsilon = 0.01 | return = 9.0\n",
      "Episode  272 :  248 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  273 :  286 steps | epsilon = 0.01 | return = 15.0\n",
      "Episode  274 :  435 steps | epsilon = 0.01 | return = 261.0\n",
      "Episode  275 :  192 steps | epsilon = 0.01 | return = 21.0\n",
      "Episode  276 :  157 steps | epsilon = 0.01 | return = 3.0\n",
      "Episode  277 : 1129 steps | epsilon = 0.01 | return = 33.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  278 :  489 steps | epsilon = 0.01 | return = 40.0\n",
      "Episode  279 :  355 steps | epsilon = 0.01 | return = 18.0\n",
      "Episode  280 :  338 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  281 :  257 steps | epsilon = 0.01 | return = -3.0\n",
      "Episode  282 :  244 steps | epsilon = 0.01 | return = 95.0\n",
      "Episode  283 :  239 steps | epsilon = 0.01 | return = 60.0\n",
      "Episode  284 :  325 steps | epsilon = 0.01 | return = 4.0\n",
      "Episode  285 : 1061 steps | epsilon = 0.01 | return = 0.0\n",
      "Episode  286 :  809 steps | epsilon = 0.01 | return = 76.0\n",
      "Episode  287 :  560 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  288 :  702 steps | epsilon = 0.01 | return = 220.0\n",
      "Episode  289 :  498 steps | epsilon = 0.01 | return = 45.0\n",
      "Episode  290 :  229 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  291 : 1479 steps | epsilon = 0.01 | return = 71.0\n",
      "Episode  292 :  773 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  293 :  894 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  294 :  915 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  295 :  517 steps | epsilon = 0.01 | return = 68.0\n",
      "Episode  296 :  206 steps | epsilon = 0.01 | return = -4.0\n",
      "Episode  297 :  554 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  298 :  259 steps | epsilon = 0.01 | return = -5.0\n",
      "Episode  299 :  367 steps | epsilon = 0.01 | return = 107.0\n",
      "Episode  300 :  428 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  301 :  984 steps | epsilon = 0.01 | return = 15.0\n",
      "Episode  302 :  360 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  303 :  342 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  304 : 1568 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  305 :  344 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  306 :  822 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  307 :  646 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  308 :  350 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  309 :  370 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  310 :  305 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  311 :  160 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  312 :  914 steps | epsilon = 0.01 | return = -4.0\n",
      "Episode  313 :  415 steps | epsilon = 0.01 | return = 229.0\n",
      "Episode  314 :  188 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  315 :  136 steps | epsilon = 0.01 | return = -4.0\n",
      "Episode  316 :  199 steps | epsilon = 0.01 | return = 30.0\n",
      "Episode  317 :  434 steps | epsilon = 0.01 | return = 125.0\n",
      "Episode  318 :  253 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  319 :  486 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  320 :  197 steps | epsilon = 0.01 | return = 27.0\n",
      "Episode  321 :  238 steps | epsilon = 0.01 | return = -5.0\n",
      "Episode  322 :  311 steps | epsilon = 0.01 | return = 149.0\n",
      "Episode  323 :  860 steps | epsilon = 0.01 | return = 2.0\n",
      "Episode  324 :  300 steps | epsilon = 0.01 | return = 28.0\n",
      "Episode  325 :  439 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  326 :  361 steps | epsilon = 0.01 | return = 79.0\n",
      "Episode  327 :  179 steps | epsilon = 0.01 | return = 25.0\n",
      "Episode  328 :  259 steps | epsilon = 0.01 | return = -1.0\n",
      "Episode  329 :  198 steps | epsilon = 0.01 | return = 37.0\n",
      "Episode  330 :  210 steps | epsilon = 0.01 | return = 61.0\n",
      "Episode  331 :  149 steps | epsilon = 0.01 | return = 3.0\n",
      "Episode  332 :  431 steps | epsilon = 0.01 | return = 3.0\n",
      "Episode  333 :  181 steps | epsilon = 0.01 | return = 33.0\n",
      "Episode  334 :  163 steps | epsilon = 0.01 | return = 11.0\n",
      "Episode  335 :  468 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  336 :  334 steps | epsilon = 0.01 | return = -5.0\n",
      "Episode  337 :  161 steps | epsilon = 0.01 | return = 0.0\n",
      "Episode  338 :  247 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  339 :  407 steps | epsilon = 0.01 | return = 142.0\n",
      "Episode  340 : 1931 steps | epsilon = 0.01 | return = 1.0\n",
      "Episode  341 :  770 steps | epsilon = 0.01 | return = 10.0\n",
      "Episode  342 :  442 steps | epsilon = 0.01 | return = -5.0\n",
      "Episode  343 :  366 steps | epsilon = 0.01 | return = 118.0\n",
      "Episode  344 :  225 steps | epsilon = 0.01 | return = -3.0\n",
      "Episode  345 :  135 steps | epsilon = 0.01 | return = -4.0\n",
      "Episode  346 :  142 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  347 :  509 steps | epsilon = 0.01 | return = 6.0\n",
      "Episode  348 :  243 steps | epsilon = 0.01 | return = -1.0\n",
      "Episode  349 :  124 steps | epsilon = 0.01 | return = -6.0\n",
      "Episode  350 :  532 steps | epsilon = 0.01 | return = 368.0\n",
      "Episode  351 :  608 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  352 :  417 steps | epsilon = 0.01 | return = -8.0\n",
      "Episode  353 :  579 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  354 :  277 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  355 :  265 steps | epsilon = 0.01 | return = -3.0\n",
      "Episode  356 :  149 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  357 :  441 steps | epsilon = 0.01 | return = 14.0\n",
      "Episode  358 :  152 steps | epsilon = 0.01 | return = -3.0\n",
      "Episode  359 :  353 steps | epsilon = 0.01 | return = -4.0\n",
      "Episode  360 :  827 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  361 :  799 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  362 :  359 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  363 :  139 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  364 :   95 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  365 :   90 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  366 :  407 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  367 :  436 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  368 :  138 steps | epsilon = 0.01 | return = -4.0\n",
      "Episode  369 :  454 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  370 :  328 steps | epsilon = 0.01 | return = -7.0\n",
      "Episode  371 :  132 steps | epsilon = 0.01 | return = -6.0\n",
      "Episode  372 :  367 steps | epsilon = 0.01 | return = -6.0\n",
      "Episode  373 :  129 steps | epsilon = 0.01 | return = -6.0\n",
      "Episode  374 :  287 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  375 :  133 steps | epsilon = 0.01 | return = -4.0\n",
      "Episode  376 :  146 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  377 :  137 steps | epsilon = 0.01 | return = -5.0\n",
      "Episode  378 :  325 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  379 :  161 steps | epsilon = 0.01 | return = 1.0\n",
      "Episode  380 :  150 steps | epsilon = 0.01 | return = -3.0\n",
      "Episode  381 :  234 steps | epsilon = 0.01 | return = -6.0\n",
      "Episode  382 :  293 steps | epsilon = 0.01 | return = 141.0\n",
      "Episode  383 :  788 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  384 :  293 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  385 :  149 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  386 :  234 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  387 :  529 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  388 :  151 steps | epsilon = 0.01 | return = 39.0\n",
      "Episode  389 :  199 steps | epsilon = 0.01 | return = 24.0\n",
      "Episode  390 :  287 steps | epsilon = 0.01 | return = 110.0\n",
      "Episode  391 :  289 steps | epsilon = 0.01 | return = 11.0\n",
      "Episode  392 :  235 steps | epsilon = 0.01 | return = 78.0\n",
      "Episode  393 :  438 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  394 :  250 steps | epsilon = 0.01 | return = 75.0\n",
      "Episode  395 :  183 steps | epsilon = 0.01 | return = 6.0\n",
      "Episode  396 :  708 steps | epsilon = 0.01 | return = -4.0\n",
      "Episode  397 :  258 steps | epsilon = 0.01 | return = -1.0\n",
      "Episode  398 :  450 steps | epsilon = 0.01 | return = 30.0\n",
      "Episode  399 :  293 steps | epsilon = 0.01 | return = -10.0\n",
      "Episode  400 :  235 steps | epsilon = 0.01 | return = 10.0\n",
      "Episode  401 :  162 steps | epsilon = 0.01 | return = -1.0\n",
      "Episode  402 :  171 steps | epsilon = 0.01 | return = 2.0\n",
      "Episode  403 :  188 steps | epsilon = 0.01 | return = 1.0\n",
      "Episode  404 :  166 steps | epsilon = 0.01 | return = -3.0\n",
      "Episode  405 :  148 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  406 :  266 steps | epsilon = 0.01 | return = 0.0\n",
      "Episode  407 :  101 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  408 :  562 steps | epsilon = 0.01 | return = 27.0\n",
      "Episode  409 :  432 steps | epsilon = 0.01 | return = -2.0\n",
      "Episode  410 :  157 steps | epsilon = 0.01 | return = -1.0\n",
      "Episode  411 :  101 steps | epsilon = 0.01 | return = 13.0\n",
      "Episode  412 :  197 steps | epsilon = 0.01 | return = 1.0\n",
      "Episode  413 :  160 steps | epsilon = 0.01 | return = -1.0\n",
      "Episode  414 :  167 steps | epsilon = 0.01 | return = 1.0\n",
      "Episode  415 :  152 steps | epsilon = 0.01 | return = -1.0\n",
      "Episode  416 :  188 steps | epsilon = 0.01 | return = 22.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  417 :  170 steps | epsilon = 0.01 | return = 15.0\n",
      "Episode  418 :  218 steps | epsilon = 0.01 | return = 38.0\n",
      "Episode  419 :  180 steps | epsilon = 0.01 | return = 7.0\n",
      "Episode  420 : 1525 steps | epsilon = 0.01 | return = 44.0\n",
      "Episode  421 :  222 steps | epsilon = 0.01 | return = 44.0\n",
      "Episode  422 :  202 steps | epsilon = 0.01 | return = 24.0\n",
      "Episode  423 :  269 steps | epsilon = 0.01 | return = 54.0\n",
      "Episode  424 :  257 steps | epsilon = 0.01 | return = 52.0\n",
      "Episode  425 :  243 steps | epsilon = 0.01 | return = 55.0\n",
      "Episode  426 :  221 steps | epsilon = 0.01 | return = 32.0\n",
      "Episode  427 :  226 steps | epsilon = 0.01 | return = 20.0\n",
      "Episode  428 :  243 steps | epsilon = 0.01 | return = 27.0\n",
      "Episode  429 :  258 steps | epsilon = 0.01 | return = 75.0\n",
      "Episode  430 :  202 steps | epsilon = 0.01 | return = 36.0\n",
      "Episode  431 :  220 steps | epsilon = 0.01 | return = 48.0\n",
      "Episode  432 :  235 steps | epsilon = 0.01 | return = 58.0\n",
      "Episode  433 :  229 steps | epsilon = 0.01 | return = 50.0\n",
      "Episode  434 :  770 steps | epsilon = 0.01 | return = 49.0\n",
      "Episode  435 :  216 steps | epsilon = 0.01 | return = 3.0\n",
      "Episode  436 :  184 steps | epsilon = 0.01 | return = 9.0\n",
      "Episode  437 :  192 steps | epsilon = 0.01 | return = 32.0\n",
      "Episode  438 : 2243 steps | epsilon = 0.01 | return = 19.0\n",
      "Episode  439 :  231 steps | epsilon = 0.01 | return = 54.0\n",
      "Episode  440 :  173 steps | epsilon = 0.01 | return = 47.0\n",
      "Episode  441 :  185 steps | epsilon = 0.01 | return = 69.0\n",
      "Episode  442 :  178 steps | epsilon = 0.01 | return = 67.0\n",
      "Episode  443 :  188 steps | epsilon = 0.01 | return = 75.0\n",
      "Episode  444 :  229 steps | epsilon = 0.01 | return = 67.0\n",
      "Episode  445 :  285 steps | epsilon = 0.01 | return = 110.0\n",
      "Episode  446 :  282 steps | epsilon = 0.01 | return = 103.0\n",
      "Episode  447 :  236 steps | epsilon = 0.01 | return = 24.0\n",
      "Episode  448 :  269 steps | epsilon = 0.01 | return = 98.0\n",
      "Episode  449 :  236 steps | epsilon = 0.01 | return = 38.0\n",
      "Episode  450 :  258 steps | epsilon = 0.01 | return = 47.0\n",
      "Episode  451 :  244 steps | epsilon = 0.01 | return = 59.0\n",
      "Episode  452 :  281 steps | epsilon = 0.01 | return = 99.0\n",
      "Episode  453 :  113 steps | epsilon = 0.01 | return = 26.0\n",
      "Episode  454 :  252 steps | epsilon = 0.01 | return = 79.0\n",
      "Episode  455 :  397 steps | epsilon = 0.01 | return = 198.0\n",
      "Episode  456 :  305 steps | epsilon = 0.01 | return = 88.0\n",
      "Episode  457 : 1259 steps | epsilon = 0.01 | return = 45.0\n",
      "Episode  458 :  152 steps | epsilon = 0.01 | return = 32.0\n",
      "Episode  459 :  260 steps | epsilon = 0.01 | return = 77.0\n",
      "Episode  460 :  208 steps | epsilon = 0.01 | return = 32.0\n",
      "Episode  461 :  267 steps | epsilon = 0.01 | return = 61.0\n",
      "Episode  462 :  270 steps | epsilon = 0.01 | return = 75.0\n",
      "Episode  463 :  264 steps | epsilon = 0.01 | return = 50.0\n",
      "Episode  464 :  111 steps | epsilon = 0.01 | return = 14.0\n",
      "Episode  465 :  178 steps | epsilon = 0.01 | return = 70.0\n",
      "Episode  466 :  166 steps | epsilon = 0.01 | return = 58.0\n",
      "Episode  467 :  249 steps | epsilon = 0.01 | return = 62.0\n",
      "Episode  468 :  193 steps | epsilon = 0.01 | return = 92.0\n",
      "Episode  469 :  241 steps | epsilon = 0.01 | return = 25.0\n",
      "Episode  470 :  372 steps | epsilon = 0.01 | return = 279.0\n",
      "Episode  471 : 1472 steps | epsilon = 0.01 | return = 1259.0\n",
      "Episode  472 :  203 steps | epsilon = 0.01 | return = 16.0\n",
      "Episode  473 :  243 steps | epsilon = 0.01 | return = 28.0\n",
      "Episode  474 :  220 steps | epsilon = 0.01 | return = 31.0\n",
      "Episode  475 :  299 steps | epsilon = 0.01 | return = 78.0\n",
      "Episode  476 :  294 steps | epsilon = 0.01 | return = 89.0\n",
      "Episode  477 :  203 steps | epsilon = 0.01 | return = 19.0\n",
      "Episode  478 :  200 steps | epsilon = 0.01 | return = 27.0\n",
      "Episode  479 :  348 steps | epsilon = 0.01 | return = 134.0\n",
      "Episode  480 :  275 steps | epsilon = 0.01 | return = 104.0\n",
      "Episode  481 : 1335 steps | epsilon = 0.01 | return = 117.0\n",
      "Episode  482 :  289 steps | epsilon = 0.01 | return = 122.0\n",
      "Episode  483 :  297 steps | epsilon = 0.01 | return = 128.0\n",
      "Episode  484 :  299 steps | epsilon = 0.01 | return = 128.0\n",
      "Episode  485 :  235 steps | epsilon = 0.01 | return = 95.0\n",
      "Episode  486 :  170 steps | epsilon = 0.01 | return = 30.0\n",
      "Episode  487 :  320 steps | epsilon = 0.01 | return = 146.0\n",
      "Episode  488 :  332 steps | epsilon = 0.01 | return = 124.0\n",
      "Episode  489 :  305 steps | epsilon = 0.01 | return = 125.0\n",
      "Episode  490 :  347 steps | epsilon = 0.01 | return = 127.0\n",
      "Episode  491 :  356 steps | epsilon = 0.01 | return = 180.0\n",
      "Episode  492 :  265 steps | epsilon = 0.01 | return = 95.0\n",
      "Episode  493 :  154 steps | epsilon = 0.01 | return = 10.0\n",
      "Episode  494 :  316 steps | epsilon = 0.01 | return = 147.0\n",
      "Episode  495 :  270 steps | epsilon = 0.01 | return = 40.0\n",
      "Episode  496 :  247 steps | epsilon = 0.01 | return = 65.0\n",
      "Episode  497 :  270 steps | epsilon = 0.01 | return = 50.0\n",
      "Episode  498 :  111 steps | epsilon = 0.01 | return = 19.0\n",
      "Episode  499 :  284 steps | epsilon = 0.01 | return = 118.0\n",
      "Episode  500 :  656 steps | epsilon = 0.01 | return = -10.0\n"
     ]
    }
   ],
   "source": [
    "episode_return_list = agent.train(env, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "257b8773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.test(env, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "14be92ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('agent.pkl', 'wb') as output:\n",
    "    pickle.dump(agent, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab7afff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('agent.pkl', 'rb') as input:\n",
    "    agent_ = pickle.load(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
